{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "We will use natural language processing to help with our analysis of the horror movie reviews. This topic is very broad and can be used for many things, for our particular case I will be using to get create sentiment analysis scores and also clustering movies. Lets kick this off by loading data from our previous script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load our pickled data, this includes a list holding reviews for our horror movies.\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "with open('../data/reviews_list.pkl', 'rb') as file:\n",
    "    reviews_list = pickle.load(file)\n",
    "\n",
    "# and my corresponding movie titles\n",
    "\n",
    "with open(\"../data/titles_list.json\", \"r\") as file:\n",
    "    titles_list = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data above contains a list of reviews for our horror movies. Each element in our list is a review, which is really a long string containing several sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ghost Ship : Sean Murphy and his crew are the top salvage exper\n"
     ]
    }
   ],
   "source": [
    "print(f'{titles_list[0]} : {reviews_list[0][0:50]}') # grabbing movie title and the first first 50 characters in our first review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text normalization\n",
    "\n",
    "Lets begin by processing our data. We start at the lowest level with an Lexical analysis, which focuses from the most basic structure dealing with words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The first step is to create features for each of our words, in NLP jargon, these are referred to as tokens. Note this can also be done at the sentence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sean', 'Murphy', 'and', 'his', 'crew', 'are', 'the', 'top', 'salvage']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords') # stopwords corpus\n",
    "\n",
    "Token_Pattern = r'\\w+'\n",
    "\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=Token_Pattern,gaps=False)\n",
    "\n",
    "#word tokenization\n",
    "tokenized_reviews = [regex_wt.tokenize(x) for x in reviews_list]\n",
    "\n",
    "tokenized_reviews[0][0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "\n",
    "Stop words, which 'don't' provide much value for our analysis are identified and removed. We need a corpus, analagous to a dictionary to help us identify stop words such as 'and','or','either'. I also supplement my stop word corpus with additional words that might not be as relevant for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ghost Ship</td>\n",
       "      <td>Sean Murphy crew top salvage experts land sea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Craft</td>\n",
       "      <td>SPOILERSI thought decent teen flick remember e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>House of 1000 Corpses</td>\n",
       "      <td>opinion House 1000 Corpses fan Fans genre Rob ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Haunting of Bly Manor</td>\n",
       "      <td>many people saying right Haunted house tales g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Attack on Titan</td>\n",
       "      <td>moment watch audiovisual masterpiece immediate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Movie  \\\n",
       "0                 Ghost Ship   \n",
       "1                  The Craft   \n",
       "2      House of 1000 Corpses   \n",
       "3  The Haunting of Bly Manor   \n",
       "4            Attack on Titan   \n",
       "\n",
       "                                              Review  \n",
       "0  Sean Murphy crew top salvage experts land sea ...  \n",
       "1  SPOILERSI thought decent teen flick remember e...  \n",
       "2  opinion House 1000 Corpses fan Fans genre Rob ...  \n",
       "3  many people saying right Haunted house tales g...  \n",
       "4  moment watch audiovisual masterpiece immediate...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function to remove stop words\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stopword_list=stopword_list + ['movie','film','horror','good','show','films','see','much','list', 'com', 'http', 'www', 'imdb',\n",
    "                               'tvd','characters','one','pretty','really','thing','ever','like','definitely','also','well']\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "stop_words = [remove_stopwords(x) for x in tokenized_reviews]\n",
    "\n",
    "normalized_reviews=[re.sub('[^A-Za-z0-9]+', ' ', x) for x in stop_words]\n",
    "normalized_reviews = [x.strip() for x in normalized_reviews]\n",
    "\n",
    "#Preview our data so far\n",
    "\n",
    "df = pd.DataFrame({'Movie': titles_list,'Review': normalized_reviews})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('../data/normalized_reviews.pkl', 'wb') as file:\n",
    "    pickle.dump(df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech Tagging\n",
    "\n",
    "We apply a shallow form of syntactic parsing, this means we only look at individual tokens, and not decompose them into phrases. We identify what parts of speech these words are, meaning are they nouns, verbs, adjectives. We combine <i>names</i> corpus to identify names plus nltk's default <i>pos_tag</i> function which acts as a secondary tagger following the initial POS tagging using the names corpus.\n",
    "\n",
    "We use the popular Penn Treebank Tagset, which uses Penn Treebank corpus training on Wall Street Journal data. Corresponding tagsets can be found here. \n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Sean', 'NNP'),\n",
       " ('Murphy', 'NNP'),\n",
       " ('crew', 'VBD'),\n",
       " ('top', 'JJ'),\n",
       " ('salvage', 'NN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.tag, nltk.data\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import names\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "\n",
    "tagger_path = 'taggers/maxent_treebank_pos_tagger/english.pickle' \n",
    "default_tagger = nltk.data.load(tagger_path)\n",
    "\n",
    "#Supplementing NLTK's default pos_tag function with name corpus\n",
    "\n",
    "mlist = names.words('male.txt')\n",
    "flist = names.words('female.txt')\n",
    "names = mlist+flist\n",
    "dict_names = dict.fromkeys(names, 'NNP')\n",
    "\n",
    "#Combine unigram and nltk's POS_Tag function\n",
    "tagger = nltk.tag.UnigramTagger(model=dict_names, backoff=default_tagger) # try BigramTagger and notice the difference\n",
    "\n",
    "normalized_tkn = [nltk.word_tokenize(x) for x in normalized_reviews]\n",
    "\n",
    "reviews = [tagger.tag(x) for x in normalized_tkn] #POS tagging\n",
    "\n",
    "reviews[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that 'Sean' from the movie Ghost Ship is identified as a NNP or a proper noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('../data/processed_reviews.pkl', 'wb') as file:\n",
    "    pickle.dump(reviews, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
