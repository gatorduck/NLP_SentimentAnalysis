{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Lets begin by loading some data, as this is required for all data science / machine learning projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Review URLs\n",
    "\n",
    "In order to collect that data we need to build a spider for webscraping. I won't go into depth regarding this portion as this is outside the scope of our topic.\n",
    "\n",
    "We use a little object oriented programming to create a spider class that will help us scrape movie review links which we will use to parse movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Selector\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "#custom spider class\n",
    "class MovieSpider( scrapy.Spider ):\n",
    "    name = \"Horror_Movie_Spider\"\n",
    "  # start_requests method\n",
    "    def start_requests( self ):\n",
    "        urls = ['https://www.imdb.com/search/title/?genres=horror&explore=title_type,genres','https://www.imdb.com/search/title/?genres=horror&start=51&explore=title_type,genres&ref_=adv_nxt']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url = url, callback=self.movie_links)\n",
    "      \n",
    "  # movie_links parse method - finds link for movie\n",
    "    def movie_links( self, response ):\n",
    "        movie_links = response.xpath('//div[@id=\"main\"]//div[@class=\"lister-item mode-advanced\"]//div[@class=\"lister-item-content\"]//h3[@class=\"lister-item-header\"]//a/@href')\n",
    "        movie_genres = movie_links\n",
    "    #extract link\n",
    "        genre_string_link = movie_genres.extract()\n",
    "        for link in genre_string_link:\n",
    "            yield response.follow(url=link, callback=self.review_links)\n",
    "  # review_links parse method - find link for review      \n",
    "    def review_links(self, response):\n",
    "        review_links = response.xpath('//div[@id=\"quicklinksBar\"]//a[3]//@href')\n",
    "        review_list = review_links.extract()\n",
    "        for link in review_list:\n",
    "            yield response.follow(url=link, callback=self.reviews)\n",
    "   # reviews parse method - pull first review           \n",
    "    def reviews(self, response):\n",
    "        relativeurl = response.xpath('//div[@class=\"lister-list\"]/div[1]//a[@class=\"title\"]/@href').extract()\n",
    "        for sel in relativeurl:\n",
    "            data = {}\n",
    "            data['url'] = response.urljoin(sel)\n",
    "            yield data            \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess() # initiate the CrawlerProcess\n",
    "\n",
    "process.crawl(MovieSpider) # tell the process which spider to use\n",
    "\n",
    "process.start() # start the crawling process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaping Reviews\n",
    "\n",
    "I have saved the reviews in a csv file to prevent from having to rerun the spider and potentially being blocked for web scraping. And also pickled the reviews and titles as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (404): https://www.imdb.com/review/rw6073713/\n",
      "Error (404): https://www.imdb.com/review/rw4236449/\n",
      "Error (404): https://www.imdb.com/review/rw6112553/\n",
      "Error (404): https://www.imdb.com/review/rw5867239/\n",
      "Error (404): https://www.imdb.com/review/rw5954517/\n",
      "Error (404): https://www.imdb.com/review/rw1621039/\n",
      "Error (404): https://www.imdb.com/review/rw5925550/\n",
      "Error (404): https://www.imdb.com/review/rw2833730/\n",
      "Error (404): https://www.imdb.com/review/rw5500399/\n",
      "Error (404): https://www.imdb.com/review/rw5489629/\n",
      "Error (404): https://www.imdb.com/review/rw6157062/\n",
      "Error (404): https://www.imdb.com/review/rw6073667/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "with open('../data/ReviewUrls.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)  # Skip header row\n",
    "    URLlist = list(reader)\n",
    "\n",
    "URLlist = [val for sublist in URLlist for val in sublist]\n",
    "\n",
    "titles_list = []\n",
    "reviews_list = []\n",
    "\n",
    "for url in URLlist:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an exception for error codes (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        results = soup.find('div', {'class': 'text show-more__control'})\n",
    "        reviews_list.append(results)\n",
    "        titles = soup.find('div', {'class': 'subpage_title_block'})\n",
    "        titles = titles.h1.a.contents\n",
    "        titles_list.append(titles)\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"Error ({e.response.status_code}): {url}\")\n",
    "\n",
    "titles_list = [val for sublist in titles_list for val in sublist]\n",
    "reviews_list = [x.get_text() for x in reviews_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del reviews_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ghost Ship',\n",
       " 'The Craft',\n",
       " 'House of 1000 Corpses',\n",
       " 'The Haunting of Bly Manor',\n",
       " 'Attack on Titan',\n",
       " 'The Stand',\n",
       " 'The Babysitter',\n",
       " 'The Shining',\n",
       " 'It',\n",
       " 'Swamp Thing',\n",
       " 'Friday the 13th',\n",
       " 'The Mortuary Collection',\n",
       " 'The Platform',\n",
       " 'The Fly',\n",
       " 'Tarantula',\n",
       " 'The Twilight Zone',\n",
       " 'Fright Night',\n",
       " 'The Haunting',\n",
       " 'Peninsula',\n",
       " 'The Last House on the Left',\n",
       " 'Legacies',\n",
       " 'You Should Have Left',\n",
       " 'Psycho',\n",
       " 'Scream 2',\n",
       " 'The VVitch: A New-England Folktale',\n",
       " 'The Haunted Mansion',\n",
       " 'The Twilight Zone',\n",
       " 'iZombie',\n",
       " 'House of Wax',\n",
       " \"Bram Stoker's Dracula\",\n",
       " 'Scream 4',\n",
       " 'Possessor',\n",
       " 'The Hunt',\n",
       " 'The Exorcist',\n",
       " 'The Turning',\n",
       " 'The Thing',\n",
       " 'Thir13en Ghosts',\n",
       " 'Alone',\n",
       " 'Alien',\n",
       " 'Overlord',\n",
       " 'Us',\n",
       " 'Chilling Adventures of Sabrina',\n",
       " 'It Chapter Two',\n",
       " 'The Originals',\n",
       " 'A Nightmare on Elm Street',\n",
       " 'Ready or Not',\n",
       " 'Van Helsing',\n",
       " 'The Cabin in the Woods',\n",
       " \"Trick 'r Treat\",\n",
       " \"Gerald's Game\",\n",
       " 'Saint Maud',\n",
       " 'Midsommar',\n",
       " 'Evil Eye',\n",
       " 'Hannibal',\n",
       " 'Sleepy Hollow',\n",
       " 'What We Do in the Shadows',\n",
       " 'The Babysitter: Killer Queen',\n",
       " 'Vampires vs. the Bronx',\n",
       " 'Scream',\n",
       " 'Get Out',\n",
       " 'The Addams Family',\n",
       " 'Doctor Sleep',\n",
       " 'Halloween',\n",
       " 'Nocturne',\n",
       " 'Black Box',\n",
       " 'The Witches',\n",
       " 'The Invisible Man',\n",
       " 'Helstrom',\n",
       " 'Monsterland',\n",
       " 'The Wolf of Snow Hollow',\n",
       " 'The Lie',\n",
       " 'Evil',\n",
       " 'The Walking Dead: World Beyond',\n",
       " 'The Vampire Diaries',\n",
       " 'Supernatural',\n",
       " 'Stranger Things',\n",
       " 'Fear the Walking Dead',\n",
       " 'The Haunting of Hill House',\n",
       " 'American Horror Story',\n",
       " 'The Walking Dead']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31864\\4248718304.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/titles_list.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitles_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('../data/titles_list.pkl', 'wb') as file:\n",
    "    pickle.dump(titles_list, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews list loaded successfully.\n"
     ]
    }
   ],
   "source": [
    " with open('../data/reviews_list.pkl', 'rb') as file:\n",
    "    try:\n",
    "        loaded_reviews_list = pickle.load(file)\n",
    "        print(\"Reviews list loaded successfully.\")\n",
    "    except EOFError:\n",
    "        print(\"Error: Reviews list is empty or corrupted.\")\n",
    "\n",
    "#with open('../data/reviews_list.pkl', 'wb') as file:\n",
    "#    pickle.dump(reviews_list, file)\n",
    "\n",
    "# Save titles_list to a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Titles list is empty or corrupted.\n"
     ]
    }
   ],
   "source": [
    "with open('../data/titles_list.pkl', 'rb') as file:\n",
    "    try:\n",
    "        loaded_titles_list = pickle.load(file)\n",
    "        print(\"Titles list loaded successfully.\")\n",
    "    except EOFError:\n",
    "        print(\"Error: Titles list is empty or corrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ghost Ship',\n",
       " 'The Craft',\n",
       " 'House of 1000 Corpses',\n",
       " 'The Haunting of Bly Manor',\n",
       " 'Attack on Titan',\n",
       " 'The Stand',\n",
       " 'The Babysitter',\n",
       " 'The Shining',\n",
       " 'It',\n",
       " 'Swamp Thing',\n",
       " 'Friday the 13th',\n",
       " 'The Mortuary Collection',\n",
       " 'The Platform',\n",
       " 'The Fly',\n",
       " 'Tarantula',\n",
       " 'The Twilight Zone',\n",
       " 'Fright Night',\n",
       " 'The Haunting',\n",
       " 'Peninsula',\n",
       " 'The Last House on the Left',\n",
       " 'Legacies',\n",
       " 'You Should Have Left',\n",
       " 'Psycho',\n",
       " 'Scream 2',\n",
       " 'The VVitch: A New-England Folktale',\n",
       " 'The Haunted Mansion',\n",
       " 'The Twilight Zone',\n",
       " 'iZombie',\n",
       " 'House of Wax',\n",
       " \"Bram Stoker's Dracula\",\n",
       " 'Scream 4',\n",
       " 'Possessor',\n",
       " 'The Hunt',\n",
       " 'The Exorcist',\n",
       " 'The Turning',\n",
       " 'The Thing',\n",
       " 'Thir13en Ghosts',\n",
       " 'Alone',\n",
       " 'Alien',\n",
       " 'Overlord',\n",
       " 'Us',\n",
       " 'Chilling Adventures of Sabrina',\n",
       " 'It Chapter Two',\n",
       " 'The Originals',\n",
       " 'A Nightmare on Elm Street',\n",
       " 'Ready or Not',\n",
       " 'Van Helsing',\n",
       " 'The Cabin in the Woods',\n",
       " \"Trick 'r Treat\",\n",
       " \"Gerald's Game\",\n",
       " 'Saint Maud',\n",
       " 'Midsommar',\n",
       " 'Evil Eye',\n",
       " 'Hannibal',\n",
       " 'Sleepy Hollow',\n",
       " 'What We Do in the Shadows',\n",
       " 'The Babysitter: Killer Queen',\n",
       " 'Vampires vs. the Bronx',\n",
       " 'Scream',\n",
       " 'Get Out',\n",
       " 'The Addams Family',\n",
       " 'Doctor Sleep',\n",
       " 'Halloween',\n",
       " 'Nocturne',\n",
       " 'Black Box',\n",
       " 'The Witches',\n",
       " 'The Invisible Man',\n",
       " 'Helstrom',\n",
       " 'Monsterland',\n",
       " 'The Wolf of Snow Hollow',\n",
       " 'The Lie',\n",
       " 'Evil',\n",
       " 'The Walking Dead: World Beyond',\n",
       " 'The Vampire Diaries',\n",
       " 'Supernatural',\n",
       " 'Stranger Things',\n",
       " 'Fear the Walking Dead',\n",
       " 'The Haunting of Hill House',\n",
       " 'American Horror Story',\n",
       " 'The Walking Dead']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Save the list to a JSON file\n",
    "with open(\"../data/titles_list.json\", \"w\") as file:\n",
    "    json.dump(titles_list, file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
