{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Lets begin by loading some data, as this is required for all data science / machine learning projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Review URLs\n",
    "\n",
    "In order to collect that data we need to build a spider for webscraping. I won't go into depth regarding this portion as this is outside the scope of our topic.\n",
    "\n",
    "We use a little object oriented programming to create a spider class that will help us scrape movie review links which we will use to parse movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Selector\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "#custom spider class\n",
    "class MovieSpider( scrapy.Spider ):\n",
    "    name = \"Horror_Movie_Spider\"\n",
    "  # start_requests method\n",
    "    def start_requests( self ):\n",
    "        urls = ['https://www.imdb.com/search/title/?genres=horror&explore=title_type,genres','https://www.imdb.com/search/title/?genres=horror&start=51&explore=title_type,genres&ref_=adv_nxt']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url = url, callback=self.movie_links)\n",
    "      \n",
    "  # movie_links parse method - finds link for movie\n",
    "    def movie_links( self, response ):\n",
    "        movie_links = response.xpath('//div[@id=\"main\"]//div[@class=\"lister-item mode-advanced\"]//div[@class=\"lister-item-content\"]//h3[@class=\"lister-item-header\"]//a/@href')\n",
    "        movie_genres = movie_links\n",
    "    #extract link\n",
    "        genre_string_link = movie_genres.extract()\n",
    "        for link in genre_string_link:\n",
    "            yield response.follow(url=link, callback=self.review_links)\n",
    "  # review_links parse method - find link for review      \n",
    "    def review_links(self, response):\n",
    "        review_links = response.xpath('//div[@id=\"quicklinksBar\"]//a[3]//@href')\n",
    "        review_list = review_links.extract()\n",
    "        for link in review_list:\n",
    "            yield response.follow(url=link, callback=self.reviews)\n",
    "   # reviews parse method - pull first review           \n",
    "    def reviews(self, response):\n",
    "        relativeurl = response.xpath('//div[@class=\"lister-list\"]/div[1]//a[@class=\"title\"]/@href').extract()\n",
    "        for sel in relativeurl:\n",
    "            data = {}\n",
    "            data['url'] = response.urljoin(sel)\n",
    "            yield data            \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess() # initiate the CrawlerProcess\n",
    "\n",
    "process.crawl(MovieSpider) # tell the process which spider to use\n",
    "\n",
    "process.start() # start the crawling process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaping Reviews\n",
    "\n",
    "I have saved the reviews in a csv file to prevent from having to rerun the spider and potentially being blocked for web scraping. And also pickled the reviews and titles as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (404): https://www.imdb.com/review/rw6073713/\n",
      "Error (404): https://www.imdb.com/review/rw4236449/\n",
      "Error (404): https://www.imdb.com/review/rw6112553/\n",
      "Error (404): https://www.imdb.com/review/rw5867239/\n",
      "Error (404): https://www.imdb.com/review/rw5954517/\n",
      "Error (404): https://www.imdb.com/review/rw1621039/\n",
      "Error (404): https://www.imdb.com/review/rw5925550/\n",
      "Error (404): https://www.imdb.com/review/rw2833730/\n",
      "Error (404): https://www.imdb.com/review/rw5500399/\n",
      "Error (404): https://www.imdb.com/review/rw5489629/\n",
      "Error (404): https://www.imdb.com/review/rw6157062/\n",
      "Error (404): https://www.imdb.com/review/rw6073667/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "with open('../data/ReviewUrls.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader, None)  # Skip header row\n",
    "    URLlist = list(reader)\n",
    "\n",
    "URLlist = [val for sublist in URLlist for val in sublist]\n",
    "\n",
    "titles_list = []\n",
    "reviews_list = []\n",
    "\n",
    "for url in URLlist:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an exception for error codes (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        results = soup.find('div', {'class': 'text show-more__control'})\n",
    "        reviews_list.append(results)\n",
    "        titles = soup.find('div', {'class': 'subpage_title_block'})\n",
    "        titles = titles.h1.a.contents\n",
    "        titles_list.append(titles)\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"Error ({e.response.status_code}): {url}\")\n",
    "\n",
    "titles_list = [val for sublist in titles_list for val in sublist]\n",
    "reviews_list = [x.get_text() for x in reviews_list]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
